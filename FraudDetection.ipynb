{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReEiuh8Xl2mL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "vuqdcYqr2WCv",
        "outputId": "35bd0d0f-3b03-4d3a-902c-48dcd1579ed1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-0f0685e5-f8bf-4b64-a5d9-64f4efa4a312\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.425966</td>\n",
              "      <td>0.960523</td>\n",
              "      <td>1.141109</td>\n",
              "      <td>-0.168252</td>\n",
              "      <td>0.420987</td>\n",
              "      <td>-0.029728</td>\n",
              "      <td>0.476201</td>\n",
              "      <td>0.260314</td>\n",
              "      <td>-0.568671</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.208254</td>\n",
              "      <td>-0.559825</td>\n",
              "      <td>-0.026398</td>\n",
              "      <td>-0.371427</td>\n",
              "      <td>-0.232794</td>\n",
              "      <td>0.105915</td>\n",
              "      <td>0.253844</td>\n",
              "      <td>0.081080</td>\n",
              "      <td>3.67</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4</td>\n",
              "      <td>1.229658</td>\n",
              "      <td>0.141004</td>\n",
              "      <td>0.045371</td>\n",
              "      <td>1.202613</td>\n",
              "      <td>0.191881</td>\n",
              "      <td>0.272708</td>\n",
              "      <td>-0.005159</td>\n",
              "      <td>0.081213</td>\n",
              "      <td>0.464960</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.167716</td>\n",
              "      <td>-0.270710</td>\n",
              "      <td>-0.154104</td>\n",
              "      <td>-0.780055</td>\n",
              "      <td>0.750137</td>\n",
              "      <td>-0.257237</td>\n",
              "      <td>0.034507</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>4.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>-0.644269</td>\n",
              "      <td>1.417964</td>\n",
              "      <td>1.074380</td>\n",
              "      <td>-0.492199</td>\n",
              "      <td>0.948934</td>\n",
              "      <td>0.428118</td>\n",
              "      <td>1.120631</td>\n",
              "      <td>-3.807864</td>\n",
              "      <td>0.615375</td>\n",
              "      <td>...</td>\n",
              "      <td>1.943465</td>\n",
              "      <td>-1.015455</td>\n",
              "      <td>0.057504</td>\n",
              "      <td>-0.649709</td>\n",
              "      <td>-0.415267</td>\n",
              "      <td>-0.051634</td>\n",
              "      <td>-1.206921</td>\n",
              "      <td>-1.085339</td>\n",
              "      <td>40.80</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>-0.894286</td>\n",
              "      <td>0.286157</td>\n",
              "      <td>-0.113192</td>\n",
              "      <td>-0.271526</td>\n",
              "      <td>2.669599</td>\n",
              "      <td>3.721818</td>\n",
              "      <td>0.370145</td>\n",
              "      <td>0.851084</td>\n",
              "      <td>-0.392048</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.073425</td>\n",
              "      <td>-0.268092</td>\n",
              "      <td>-0.204233</td>\n",
              "      <td>1.011592</td>\n",
              "      <td>0.373205</td>\n",
              "      <td>-0.384157</td>\n",
              "      <td>0.011747</td>\n",
              "      <td>0.142404</td>\n",
              "      <td>93.20</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>-0.338262</td>\n",
              "      <td>1.119593</td>\n",
              "      <td>1.044367</td>\n",
              "      <td>-0.222187</td>\n",
              "      <td>0.499361</td>\n",
              "      <td>-0.246761</td>\n",
              "      <td>0.651583</td>\n",
              "      <td>0.069539</td>\n",
              "      <td>-0.736727</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.246914</td>\n",
              "      <td>-0.633753</td>\n",
              "      <td>-0.120794</td>\n",
              "      <td>-0.385050</td>\n",
              "      <td>-0.069733</td>\n",
              "      <td>0.094199</td>\n",
              "      <td>0.246219</td>\n",
              "      <td>0.083076</td>\n",
              "      <td>3.68</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>1.449044</td>\n",
              "      <td>-1.176339</td>\n",
              "      <td>0.913860</td>\n",
              "      <td>-1.375667</td>\n",
              "      <td>-1.971383</td>\n",
              "      <td>-0.629152</td>\n",
              "      <td>-1.423236</td>\n",
              "      <td>0.048456</td>\n",
              "      <td>-1.720408</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009302</td>\n",
              "      <td>0.313894</td>\n",
              "      <td>0.027740</td>\n",
              "      <td>0.500512</td>\n",
              "      <td>0.251367</td>\n",
              "      <td>-0.129478</td>\n",
              "      <td>0.042850</td>\n",
              "      <td>0.016253</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10</td>\n",
              "      <td>0.384978</td>\n",
              "      <td>0.616109</td>\n",
              "      <td>-0.874300</td>\n",
              "      <td>-0.094019</td>\n",
              "      <td>2.924584</td>\n",
              "      <td>3.317027</td>\n",
              "      <td>0.470455</td>\n",
              "      <td>0.538247</td>\n",
              "      <td>-0.558895</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049924</td>\n",
              "      <td>0.238422</td>\n",
              "      <td>0.009130</td>\n",
              "      <td>0.996710</td>\n",
              "      <td>-0.767315</td>\n",
              "      <td>-0.492208</td>\n",
              "      <td>0.042472</td>\n",
              "      <td>-0.054337</td>\n",
              "      <td>9.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>10</td>\n",
              "      <td>1.249999</td>\n",
              "      <td>-1.221637</td>\n",
              "      <td>0.383930</td>\n",
              "      <td>-1.234899</td>\n",
              "      <td>-1.485419</td>\n",
              "      <td>-0.753230</td>\n",
              "      <td>-0.689405</td>\n",
              "      <td>-0.227487</td>\n",
              "      <td>-2.094011</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231809</td>\n",
              "      <td>-0.483285</td>\n",
              "      <td>0.084668</td>\n",
              "      <td>0.392831</td>\n",
              "      <td>0.161135</td>\n",
              "      <td>-0.354990</td>\n",
              "      <td>0.026416</td>\n",
              "      <td>0.042422</td>\n",
              "      <td>121.50</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>11</td>\n",
              "      <td>1.069374</td>\n",
              "      <td>0.287722</td>\n",
              "      <td>0.828613</td>\n",
              "      <td>2.712520</td>\n",
              "      <td>-0.178398</td>\n",
              "      <td>0.337544</td>\n",
              "      <td>-0.096717</td>\n",
              "      <td>0.115982</td>\n",
              "      <td>-0.221083</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036876</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.071407</td>\n",
              "      <td>0.104744</td>\n",
              "      <td>0.548265</td>\n",
              "      <td>0.104094</td>\n",
              "      <td>0.021491</td>\n",
              "      <td>0.021293</td>\n",
              "      <td>27.50</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>12</td>\n",
              "      <td>-2.791855</td>\n",
              "      <td>-0.327771</td>\n",
              "      <td>1.641750</td>\n",
              "      <td>1.767473</td>\n",
              "      <td>-0.136588</td>\n",
              "      <td>0.807596</td>\n",
              "      <td>-0.422911</td>\n",
              "      <td>-1.907107</td>\n",
              "      <td>0.755713</td>\n",
              "      <td>...</td>\n",
              "      <td>1.151663</td>\n",
              "      <td>0.222182</td>\n",
              "      <td>1.020586</td>\n",
              "      <td>0.028317</td>\n",
              "      <td>-0.232746</td>\n",
              "      <td>-0.235557</td>\n",
              "      <td>-0.164778</td>\n",
              "      <td>-0.030154</td>\n",
              "      <td>58.80</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>12</td>\n",
              "      <td>-0.752417</td>\n",
              "      <td>0.345485</td>\n",
              "      <td>2.057323</td>\n",
              "      <td>-1.468643</td>\n",
              "      <td>-1.158394</td>\n",
              "      <td>-0.077850</td>\n",
              "      <td>-0.608581</td>\n",
              "      <td>0.003603</td>\n",
              "      <td>-0.436167</td>\n",
              "      <td>...</td>\n",
              "      <td>0.499625</td>\n",
              "      <td>1.353650</td>\n",
              "      <td>-0.256573</td>\n",
              "      <td>-0.065084</td>\n",
              "      <td>-0.039124</td>\n",
              "      <td>-0.087086</td>\n",
              "      <td>-0.180998</td>\n",
              "      <td>0.129394</td>\n",
              "      <td>15.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>12</td>\n",
              "      <td>1.103215</td>\n",
              "      <td>-0.040296</td>\n",
              "      <td>1.267332</td>\n",
              "      <td>1.289091</td>\n",
              "      <td>-0.735997</td>\n",
              "      <td>0.288069</td>\n",
              "      <td>-0.586057</td>\n",
              "      <td>0.189380</td>\n",
              "      <td>0.782333</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024612</td>\n",
              "      <td>0.196002</td>\n",
              "      <td>0.013802</td>\n",
              "      <td>0.103758</td>\n",
              "      <td>0.364298</td>\n",
              "      <td>-0.382261</td>\n",
              "      <td>0.092809</td>\n",
              "      <td>0.037051</td>\n",
              "      <td>12.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>13</td>\n",
              "      <td>-0.436905</td>\n",
              "      <td>0.918966</td>\n",
              "      <td>0.924591</td>\n",
              "      <td>-0.727219</td>\n",
              "      <td>0.915679</td>\n",
              "      <td>-0.127867</td>\n",
              "      <td>0.707642</td>\n",
              "      <td>0.087962</td>\n",
              "      <td>-0.665271</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.194796</td>\n",
              "      <td>-0.672638</td>\n",
              "      <td>-0.156858</td>\n",
              "      <td>-0.888386</td>\n",
              "      <td>-0.342413</td>\n",
              "      <td>-0.049027</td>\n",
              "      <td>0.079692</td>\n",
              "      <td>0.131024</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>14</td>\n",
              "      <td>-5.401258</td>\n",
              "      <td>-5.450148</td>\n",
              "      <td>1.186305</td>\n",
              "      <td>1.736239</td>\n",
              "      <td>3.049106</td>\n",
              "      <td>-1.763406</td>\n",
              "      <td>-1.559738</td>\n",
              "      <td>0.160842</td>\n",
              "      <td>1.233090</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.503600</td>\n",
              "      <td>0.984460</td>\n",
              "      <td>2.458589</td>\n",
              "      <td>0.042119</td>\n",
              "      <td>-0.481631</td>\n",
              "      <td>-0.621272</td>\n",
              "      <td>0.392053</td>\n",
              "      <td>0.949594</td>\n",
              "      <td>46.80</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>15</td>\n",
              "      <td>1.492936</td>\n",
              "      <td>-1.029346</td>\n",
              "      <td>0.454795</td>\n",
              "      <td>-1.438026</td>\n",
              "      <td>-1.555434</td>\n",
              "      <td>-0.720961</td>\n",
              "      <td>-1.080664</td>\n",
              "      <td>-0.053127</td>\n",
              "      <td>-1.978682</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.177650</td>\n",
              "      <td>-0.175074</td>\n",
              "      <td>0.040002</td>\n",
              "      <td>0.295814</td>\n",
              "      <td>0.332931</td>\n",
              "      <td>-0.220385</td>\n",
              "      <td>0.022298</td>\n",
              "      <td>0.007602</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>16</td>\n",
              "      <td>0.694885</td>\n",
              "      <td>-1.361819</td>\n",
              "      <td>1.029221</td>\n",
              "      <td>0.834159</td>\n",
              "      <td>-1.191209</td>\n",
              "      <td>1.309109</td>\n",
              "      <td>-0.878586</td>\n",
              "      <td>0.445290</td>\n",
              "      <td>-0.446196</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.295583</td>\n",
              "      <td>-0.571955</td>\n",
              "      <td>-0.050881</td>\n",
              "      <td>-0.304215</td>\n",
              "      <td>0.072001</td>\n",
              "      <td>-0.422234</td>\n",
              "      <td>0.086553</td>\n",
              "      <td>0.063499</td>\n",
              "      <td>231.71</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>17</td>\n",
              "      <td>0.962496</td>\n",
              "      <td>0.328461</td>\n",
              "      <td>-0.171479</td>\n",
              "      <td>2.109204</td>\n",
              "      <td>1.129566</td>\n",
              "      <td>1.696038</td>\n",
              "      <td>0.107712</td>\n",
              "      <td>0.521502</td>\n",
              "      <td>-1.191311</td>\n",
              "      <td>...</td>\n",
              "      <td>0.143997</td>\n",
              "      <td>0.402492</td>\n",
              "      <td>-0.048508</td>\n",
              "      <td>-1.371866</td>\n",
              "      <td>0.390814</td>\n",
              "      <td>0.199964</td>\n",
              "      <td>0.016371</td>\n",
              "      <td>-0.014605</td>\n",
              "      <td>34.09</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>18</td>\n",
              "      <td>1.166616</td>\n",
              "      <td>0.502120</td>\n",
              "      <td>-0.067300</td>\n",
              "      <td>2.261569</td>\n",
              "      <td>0.428804</td>\n",
              "      <td>0.089474</td>\n",
              "      <td>0.241147</td>\n",
              "      <td>0.138082</td>\n",
              "      <td>-0.989162</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018702</td>\n",
              "      <td>-0.061972</td>\n",
              "      <td>-0.103855</td>\n",
              "      <td>-0.370415</td>\n",
              "      <td>0.603200</td>\n",
              "      <td>0.108556</td>\n",
              "      <td>-0.040521</td>\n",
              "      <td>-0.011418</td>\n",
              "      <td>2.28</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>18</td>\n",
              "      <td>0.247491</td>\n",
              "      <td>0.277666</td>\n",
              "      <td>1.185471</td>\n",
              "      <td>-0.092603</td>\n",
              "      <td>-1.314394</td>\n",
              "      <td>-0.150116</td>\n",
              "      <td>-0.946365</td>\n",
              "      <td>-1.617935</td>\n",
              "      <td>1.544071</td>\n",
              "      <td>...</td>\n",
              "      <td>1.650180</td>\n",
              "      <td>0.200454</td>\n",
              "      <td>-0.185353</td>\n",
              "      <td>0.423073</td>\n",
              "      <td>0.820591</td>\n",
              "      <td>-0.227632</td>\n",
              "      <td>0.336634</td>\n",
              "      <td>0.250475</td>\n",
              "      <td>22.75</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>22</td>\n",
              "      <td>-1.946525</td>\n",
              "      <td>-0.044901</td>\n",
              "      <td>-0.405570</td>\n",
              "      <td>-1.013057</td>\n",
              "      <td>2.941968</td>\n",
              "      <td>2.955053</td>\n",
              "      <td>-0.063063</td>\n",
              "      <td>0.855546</td>\n",
              "      <td>0.049967</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.579526</td>\n",
              "      <td>-0.799229</td>\n",
              "      <td>0.870300</td>\n",
              "      <td>0.983421</td>\n",
              "      <td>0.321201</td>\n",
              "      <td>0.149650</td>\n",
              "      <td>0.707519</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f0685e5-f8bf-4b64-a5d9-64f4efa4a312')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0f0685e5-f8bf-4b64-a5d9-64f4efa4a312 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0f0685e5-f8bf-4b64-a5d9-64f4efa4a312');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2ce79685-c276-4f57-bb42-dc3a10805772\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2ce79685-c276-4f57-bb42-dc3a10805772')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2ce79685-c276-4f57-bb42-dc3a10805772 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    Time        V1        V2        V3        V4        V5        V6  \\\n",
              "0      0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
              "1      0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
              "2      1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
              "3      1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
              "4      2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
              "5      2 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
              "6      4  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
              "7      7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
              "8      7 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
              "9      9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
              "10    10  1.449044 -1.176339  0.913860 -1.375667 -1.971383 -0.629152   \n",
              "11    10  0.384978  0.616109 -0.874300 -0.094019  2.924584  3.317027   \n",
              "12    10  1.249999 -1.221637  0.383930 -1.234899 -1.485419 -0.753230   \n",
              "13    11  1.069374  0.287722  0.828613  2.712520 -0.178398  0.337544   \n",
              "14    12 -2.791855 -0.327771  1.641750  1.767473 -0.136588  0.807596   \n",
              "15    12 -0.752417  0.345485  2.057323 -1.468643 -1.158394 -0.077850   \n",
              "16    12  1.103215 -0.040296  1.267332  1.289091 -0.735997  0.288069   \n",
              "17    13 -0.436905  0.918966  0.924591 -0.727219  0.915679 -0.127867   \n",
              "18    14 -5.401258 -5.450148  1.186305  1.736239  3.049106 -1.763406   \n",
              "19    15  1.492936 -1.029346  0.454795 -1.438026 -1.555434 -0.720961   \n",
              "20    16  0.694885 -1.361819  1.029221  0.834159 -1.191209  1.309109   \n",
              "21    17  0.962496  0.328461 -0.171479  2.109204  1.129566  1.696038   \n",
              "22    18  1.166616  0.502120 -0.067300  2.261569  0.428804  0.089474   \n",
              "23    18  0.247491  0.277666  1.185471 -0.092603 -1.314394 -0.150116   \n",
              "24    22 -1.946525 -0.044901 -0.405570 -1.013057  2.941968  2.955053   \n",
              "\n",
              "          V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
              "0   0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
              "1  -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
              "2   0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
              "3   0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
              "4   0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
              "5   0.476201  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427   \n",
              "6  -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055   \n",
              "7   1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709   \n",
              "8   0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592   \n",
              "9   0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050   \n",
              "10 -1.423236  0.048456 -1.720408  ... -0.009302  0.313894  0.027740  0.500512   \n",
              "11  0.470455  0.538247 -0.558895  ...  0.049924  0.238422  0.009130  0.996710   \n",
              "12 -0.689405 -0.227487 -2.094011  ... -0.231809 -0.483285  0.084668  0.392831   \n",
              "13 -0.096717  0.115982 -0.221083  ... -0.036876  0.074412 -0.071407  0.104744   \n",
              "14 -0.422911 -1.907107  0.755713  ...  1.151663  0.222182  1.020586  0.028317   \n",
              "15 -0.608581  0.003603 -0.436167  ...  0.499625  1.353650 -0.256573 -0.065084   \n",
              "16 -0.586057  0.189380  0.782333  ... -0.024612  0.196002  0.013802  0.103758   \n",
              "17  0.707642  0.087962 -0.665271  ... -0.194796 -0.672638 -0.156858 -0.888386   \n",
              "18 -1.559738  0.160842  1.233090  ... -0.503600  0.984460  2.458589  0.042119   \n",
              "19 -1.080664 -0.053127 -1.978682  ... -0.177650 -0.175074  0.040002  0.295814   \n",
              "20 -0.878586  0.445290 -0.446196  ... -0.295583 -0.571955 -0.050881 -0.304215   \n",
              "21  0.107712  0.521502 -1.191311  ...  0.143997  0.402492 -0.048508 -1.371866   \n",
              "22  0.241147  0.138082 -0.989162  ...  0.018702 -0.061972 -0.103855 -0.370415   \n",
              "23 -0.946365 -1.617935  1.544071  ...  1.650180  0.200454 -0.185353  0.423073   \n",
              "24 -0.063063  0.855546  0.049967  ... -0.579526 -0.799229  0.870300  0.983421   \n",
              "\n",
              "         V25       V26       V27       V28  Amount  Class  \n",
              "0   0.128539 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
              "1   0.167170  0.125895 -0.008983  0.014724    2.69    0.0  \n",
              "2  -0.327642 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
              "3   0.647376 -0.221929  0.062723  0.061458  123.50    0.0  \n",
              "4  -0.206010  0.502292  0.219422  0.215153   69.99    0.0  \n",
              "5  -0.232794  0.105915  0.253844  0.081080    3.67    0.0  \n",
              "6   0.750137 -0.257237  0.034507  0.005168    4.99    0.0  \n",
              "7  -0.415267 -0.051634 -1.206921 -1.085339   40.80    0.0  \n",
              "8   0.373205 -0.384157  0.011747  0.142404   93.20    0.0  \n",
              "9  -0.069733  0.094199  0.246219  0.083076    3.68    0.0  \n",
              "10  0.251367 -0.129478  0.042850  0.016253    7.80    0.0  \n",
              "11 -0.767315 -0.492208  0.042472 -0.054337    9.99    0.0  \n",
              "12  0.161135 -0.354990  0.026416  0.042422  121.50    0.0  \n",
              "13  0.548265  0.104094  0.021491  0.021293   27.50    0.0  \n",
              "14 -0.232746 -0.235557 -0.164778 -0.030154   58.80    0.0  \n",
              "15 -0.039124 -0.087086 -0.180998  0.129394   15.99    0.0  \n",
              "16  0.364298 -0.382261  0.092809  0.037051   12.99    0.0  \n",
              "17 -0.342413 -0.049027  0.079692  0.131024    0.89    0.0  \n",
              "18 -0.481631 -0.621272  0.392053  0.949594   46.80    0.0  \n",
              "19  0.332931 -0.220385  0.022298  0.007602    5.00    0.0  \n",
              "20  0.072001 -0.422234  0.086553  0.063499  231.71    0.0  \n",
              "21  0.390814  0.199964  0.016371 -0.014605   34.09    0.0  \n",
              "22  0.603200  0.108556 -0.040521 -0.011418    2.28    0.0  \n",
              "23  0.820591 -0.227632  0.336634  0.250475   22.75    0.0  \n",
              "24  0.321201  0.149650  0.707519  0.014600    0.89    0.0  \n",
              "\n",
              "[25 rows x 31 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# loading the dataset to a Pandas DataFrame\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "df.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3UOZssx395Z",
        "outputId": "398a201e-6520-4c2f-e57b-c715cc7fec39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11959 entries, 0 to 11958\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Time    11959 non-null  int64  \n",
            " 1   V1      11959 non-null  float64\n",
            " 2   V2      11959 non-null  float64\n",
            " 3   V3      11959 non-null  float64\n",
            " 4   V4      11959 non-null  float64\n",
            " 5   V5      11959 non-null  float64\n",
            " 6   V6      11959 non-null  float64\n",
            " 7   V7      11959 non-null  float64\n",
            " 8   V8      11959 non-null  float64\n",
            " 9   V9      11959 non-null  float64\n",
            " 10  V10     11959 non-null  float64\n",
            " 11  V11     11959 non-null  float64\n",
            " 12  V12     11959 non-null  float64\n",
            " 13  V13     11959 non-null  float64\n",
            " 14  V14     11959 non-null  float64\n",
            " 15  V15     11959 non-null  float64\n",
            " 16  V16     11959 non-null  float64\n",
            " 17  V17     11959 non-null  float64\n",
            " 18  V18     11959 non-null  float64\n",
            " 19  V19     11959 non-null  float64\n",
            " 20  V20     11958 non-null  float64\n",
            " 21  V21     11958 non-null  float64\n",
            " 22  V22     11958 non-null  float64\n",
            " 23  V23     11958 non-null  float64\n",
            " 24  V24     11958 non-null  float64\n",
            " 25  V25     11958 non-null  float64\n",
            " 26  V26     11958 non-null  float64\n",
            " 27  V27     11958 non-null  float64\n",
            " 28  V28     11958 non-null  float64\n",
            " 29  Amount  11958 non-null  float64\n",
            " 30  Class   11958 non-null  float64\n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 2.8 MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJE3ZMdWNCQp",
        "outputId": "d34f9a6a-9283-43e8-c6c9-3151556fe5ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Time        V1        V2        V3        V4        V5        V6  \\\n",
            "0          0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
            "1          0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
            "2          1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
            "3          1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
            "4          2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
            "...      ...       ...       ...       ...       ...       ...       ...   \n",
            "11953  20625 -0.653155  0.372141  2.209254 -2.151937  0.451189  0.822337   \n",
            "11954  20631  1.504204 -0.411728  0.200090 -0.778753 -0.442232 -0.119677   \n",
            "11955  20636  1.134994  0.096340  0.277921  0.319692  0.742800  1.611803   \n",
            "11956  20638 -6.305012  3.944886 -4.707362  1.539602 -3.934785 -1.730565   \n",
            "11957  20638  1.161960 -0.398297  1.123732 -0.474237 -1.226667 -0.519325   \n",
            "\n",
            "             V7        V8        V9  ...       V21       V22       V23  \\\n",
            "0      0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
            "1     -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
            "2      0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
            "3      0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
            "4      0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
            "...         ...       ...       ...  ...       ...       ...       ...   \n",
            "11953  0.267881  0.083443  2.295254  ... -0.149850  0.151803 -0.505597   \n",
            "11954 -0.782660 -0.165178  0.691819  ... -0.136231 -0.217274 -0.143260   \n",
            "11955 -0.458649  0.390012  1.424541  ... -0.395605 -0.743542  0.222256   \n",
            "11956 -2.104936  3.843447  0.863458  ...  0.073140 -0.039935 -0.108896   \n",
            "11957 -0.804179  0.070134  3.262926  ... -0.121191  0.097255  0.050903   \n",
            "\n",
            "            V24       V25       V26       V27       V28  Amount  Class  \n",
            "0      0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
            "1     -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69    0.0  \n",
            "2     -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
            "3     -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50    0.0  \n",
            "4      0.141267 -0.206010  0.502292  0.219422  0.215153   69.99    0.0  \n",
            "...         ...       ...       ...       ...       ...     ...    ...  \n",
            "11953 -1.181476  0.535504 -0.759489 -0.068433 -0.176549   11.85    0.0  \n",
            "11954 -1.057332  0.529188 -0.235062 -0.012089  0.000905    9.00    0.0  \n",
            "11955 -1.859104 -0.109777  0.279049  0.012398 -0.009090    0.99    0.0  \n",
            "11956  0.691434 -0.261979 -0.447540  0.212900 -0.031021   89.99    0.0  \n",
            "11957  0.330479  0.315692 -0.712765  0.073836  0.028055   11.85    0.0  \n",
            "\n",
            "[11906 rows x 31 columns]\n",
            "        Time         V1        V2         V3         V4         V5        V6  \\\n",
            "541      406  -2.312227  1.951992  -1.609851   3.997906  -0.522188 -1.426545   \n",
            "623      472  -3.043541 -3.157307   1.088463   2.288644   1.359805 -1.064823   \n",
            "4920    4462  -2.303350  1.759247  -0.359745   2.330243  -0.821628 -0.075788   \n",
            "6108    6986  -4.397974  1.358367  -2.592844   2.679787  -1.128131 -1.706536   \n",
            "6329    7519   1.234235  3.019740  -4.304597   4.732795   3.624201 -1.357746   \n",
            "6331    7526   0.008430  4.137837  -6.240697   6.675732   0.768307 -3.353060   \n",
            "6334    7535   0.026779  4.132464  -6.560600   6.348557   1.329666 -2.513479   \n",
            "6336    7543   0.329594  3.712889  -5.775935   6.078266   1.667359 -2.420168   \n",
            "6338    7551   0.316459  3.809076  -5.615159   6.047445   1.554026 -2.651353   \n",
            "6427    7610   0.725646  2.300894  -5.329976   4.007683  -1.730411 -1.732193   \n",
            "6446    7672   0.702710  2.426433  -5.234513   4.416661  -2.170806 -2.667554   \n",
            "6472    7740   1.023874  2.001485  -4.769752   3.819195  -1.271754 -1.734662   \n",
            "6529    7891  -1.585505  3.261585  -4.137422   2.357096  -1.405043 -1.879437   \n",
            "6609    8090  -1.783229  3.402794  -3.822742   2.625368  -1.976415 -2.731689   \n",
            "6641    8169   0.857321  4.093912  -7.423894   7.380245   0.973366 -2.730762   \n",
            "6717    8408  -1.813280  4.917851  -5.926130   5.701500   1.204393 -3.035138   \n",
            "6719    8415  -0.251471  4.313523  -6.891438   6.796797   0.616297 -2.966327   \n",
            "6734    8451   0.314597  2.660670  -5.920037   4.522500  -2.315027 -2.278352   \n",
            "6774    8528   0.447396  2.481954  -5.660814   4.455923  -2.443780 -2.185040   \n",
            "6820    8614  -2.169929  3.639654  -4.508498   2.730668  -2.122693 -2.341017   \n",
            "6870    8757  -1.863756  3.442644  -4.468260   2.805336  -2.118412 -2.332285   \n",
            "6882    8808  -4.617217  1.695694  -3.114372   4.328199  -1.873257 -0.989908   \n",
            "6899    8878  -2.661802  5.856393  -7.653616   6.379742  -0.060712 -3.131550   \n",
            "6903    8886  -2.535852  5.793644  -7.618463   6.395830  -0.065210 -3.136372   \n",
            "6971    9064  -3.499108  0.258555  -4.489558   4.853894  -6.974522  3.628382   \n",
            "8296   11080  -2.125490  5.973556 -11.034727   9.007147  -1.689451 -2.854415   \n",
            "8312   11092   0.378275  3.914797  -5.726872   6.094141   1.698875 -2.807314   \n",
            "8335   11131  -1.426623  4.141986  -9.804103   6.666273  -4.749527 -2.073129   \n",
            "8615   11629  -3.891192  7.098916 -11.426467   8.607557  -2.065706 -2.985288   \n",
            "8617   11635   0.919137  4.199633  -7.535607   7.426940   1.118215 -2.886722   \n",
            "8842   12093  -4.696795  2.693867  -4.475133   5.467685  -1.556758 -1.549420   \n",
            "8845   12095  -4.727713  3.044469  -5.598354   5.928191  -2.190770 -1.529323   \n",
            "8972   12393  -4.064005  3.100935  -1.188498   3.264633  -1.903562  0.320351   \n",
            "9035   12597  -2.589617  7.016714 -13.705407  10.343228  -2.954461 -3.055116   \n",
            "9179   13126  -2.880042  5.225442 -11.063330   6.689951  -5.759924 -2.244031   \n",
            "9252   13323  -5.454362  8.287421 -12.752811   8.594342  -3.106002 -3.179949   \n",
            "9487   14073  -4.153014  8.204797 -15.031714  10.330100  -3.994426 -3.250013   \n",
            "9509   14152  -4.710529  8.636214 -15.496222  10.313349  -4.351341 -3.322689   \n",
            "10204  15817  -4.641893  2.902086  -1.572939   2.507299  -0.871783 -1.040903   \n",
            "10484  17187   1.088375  0.898474   0.394684   3.170258   0.175739 -0.221981   \n",
            "10497  17220   1.189784  0.942289   0.082334   3.024050   0.412406 -0.214415   \n",
            "10498  17230  -0.469327  1.111453   2.041003   1.731595   0.135147 -0.093625   \n",
            "10568  17520  -5.268053  9.067613 -15.960728  10.296603  -4.708241 -3.395375   \n",
            "10630  17838  -5.187878  6.967709 -13.510931   8.617895 -11.214422  0.672248   \n",
            "10690  18088 -12.224021  3.854150 -12.466766   9.648311  -2.726961 -4.445610   \n",
            "10801  18399 -14.474437  6.503185 -17.712632  11.270352  -4.150142 -3.372098   \n",
            "10891  18675 -12.339603  4.488267 -16.587073  10.107274 -10.420199  0.130670   \n",
            "10897  18690 -15.398845  7.472324 -19.026912  11.165526  -6.893856 -2.120937   \n",
            "11343  19762 -14.179165  7.421370 -21.405836  11.927512  -7.974281 -2.202710   \n",
            "11710  20011 -14.724627  7.875157 -21.872317  11.906170  -8.348734 -2.262846   \n",
            "11841  20332 -15.271362  8.326581 -22.338591  11.885313  -8.721334 -2.324307   \n",
            "11880  20451 -15.819179  8.775997 -22.804686  11.864868  -9.092361 -2.386893   \n",
            "\n",
            "              V7        V8        V9  ...       V21       V22        V23  \\\n",
            "541    -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049  -0.465211   \n",
            "623     0.325574 -0.067794 -0.270953  ...  0.661696  0.435477   1.375966   \n",
            "4920    0.562320 -0.399147 -0.238253  ... -0.294166 -0.932391   0.172726   \n",
            "6108   -3.496197 -0.248778 -0.247768  ...  0.573574  0.176968  -0.436207   \n",
            "6329    1.713445 -0.496358 -1.282858  ... -0.379068 -0.704181  -0.656805   \n",
            "6331   -1.631735  0.154612 -2.795892  ...  0.364514 -0.608057  -0.539528   \n",
            "6334   -1.689102  0.303253 -3.139409  ...  0.370509 -0.576752  -0.669605   \n",
            "6336   -0.812891  0.133080 -2.214311  ...  0.156617 -0.652450  -0.551572   \n",
            "6338   -0.746579  0.055586 -2.678679  ...  0.208828 -0.511747  -0.583813   \n",
            "6427   -3.968593  1.063728 -0.486097  ...  0.589669  0.109541   0.601045   \n",
            "6446   -3.878088  0.911337 -0.166199  ...  0.551180 -0.009802   0.721698   \n",
            "6472   -3.059245  0.889805  0.415382  ...  0.343283 -0.054196   0.709654   \n",
            "6529   -3.513687  1.515607 -1.207166  ...  0.501543 -0.546869  -0.076584   \n",
            "6609   -3.430559  1.413204 -0.776941  ...  0.454032 -0.577526   0.045967   \n",
            "6641   -1.496497  0.543015 -2.351190  ...  0.375026  0.145400   0.240603   \n",
            "6717   -1.713402  0.561257 -3.796354  ...  0.615642 -0.406427  -0.737018   \n",
            "6719   -2.436653  0.489328 -3.371639  ...  0.536892 -0.546126  -0.605240   \n",
            "6734   -4.684054  1.202270 -0.694696  ...  0.743314  0.064038   0.677842   \n",
            "6774   -4.716143  1.249803 -0.718326  ...  0.756053  0.140168   0.665411   \n",
            "6820   -4.235253  1.703538 -1.305279  ...  0.645103 -0.503529  -0.000523   \n",
            "6870   -4.261237  1.701682 -1.439396  ...  0.667927 -0.516242  -0.012218   \n",
            "6882   -4.577265  0.472216  0.472017  ...  0.481830  0.146023   0.117039   \n",
            "6899   -3.103570  1.778492 -3.831154  ...  0.734775 -0.435901  -0.384766   \n",
            "6903   -3.104557  1.823233 -3.878658  ...  0.716720 -0.448060  -0.402407   \n",
            "6971    5.431271 -1.946734 -0.775680  ... -1.052368  0.204817  -2.119007   \n",
            "8296   -7.810441  2.030870 -5.902828  ...  1.646518 -0.278485  -0.664841   \n",
            "8312   -0.591118 -0.123496 -2.530713  ...  0.149896 -0.601967  -0.613724   \n",
            "8335  -10.089931  2.791345 -3.249516  ...  1.865679  0.407809   0.605809   \n",
            "8615   -8.138589  2.973928 -6.272790  ...  1.757085 -0.189709  -0.508629   \n",
            "8617   -1.341036  0.363933 -2.203224  ...  0.316094  0.055179   0.210692   \n",
            "8842   -4.104215  0.553934 -1.498468  ...  0.573898 -0.080163   0.318408   \n",
            "8845   -4.487422  0.916392 -1.307010  ...  0.650988  0.254983   0.628843   \n",
            "8972   -0.954940 -3.277535  2.820829  ...  1.688665 -0.078845   0.193731   \n",
            "9035   -9.301289  3.349573 -5.654212  ...  1.887738  0.333998   0.287659   \n",
            "9179  -11.199975  4.014722 -3.429304  ...  2.002883  0.351102   0.795255   \n",
            "9252   -9.252794  4.245062 -6.329801  ...  1.846165 -0.267172  -0.310804   \n",
            "9487  -10.415698  4.620804 -5.711248  ...  1.976988  0.256510   0.485908   \n",
            "9509  -10.788373  5.060381 -5.689311  ...  1.990545  0.223785   0.554408   \n",
            "10204  -1.593901 -3.254905  1.908963  ...  1.963597 -0.217414  -0.549340   \n",
            "10484  -0.022989 -0.010874  0.860044  ... -0.423554 -0.800852   0.077614   \n",
            "10497   0.053558 -0.110353  0.883798  ... -0.502636 -1.047398  -0.056752   \n",
            "10498   0.266155  0.082988  0.580255  ...  0.159387  0.592670  -0.053596   \n",
            "10568 -11.161057  5.499963 -5.667376  ...  2.004110  0.191058   0.622928   \n",
            "10630  -9.462533  5.328704 -4.897006  ...  2.086083  0.760190   0.716806   \n",
            "10690 -21.922811  0.320792 -4.433162  ... -1.159830 -1.504119 -19.254328   \n",
            "10801 -16.535807 -1.443947 -6.815273  ... -2.475962  0.342391  -3.564508   \n",
            "10891 -15.600323 -1.157696 -5.304631  ... -2.089610  1.745315   1.376816   \n",
            "10897 -14.913330 -0.721214 -7.175097  ... -2.444884  0.727495  -0.345078   \n",
            "11343 -15.471612 -0.356595 -6.380125  ... -2.366836  1.130955   0.991153   \n",
            "11710 -15.833443  0.077874 -6.356833  ... -2.362345  1.099557   1.037199   \n",
            "11841 -16.196419  0.512882 -6.333685  ... -2.356896  1.068019   1.085617   \n",
            "11880 -16.560368  0.948349 -6.310658  ... -2.350634  1.036362   1.136051   \n",
            "\n",
            "            V24       V25       V26       V27       V28   Amount  Class  \n",
            "541    0.320198  0.044519  0.177840  0.261145 -0.143276     0.00    1.0  \n",
            "623   -0.293803  0.279798 -0.145362 -0.252773  0.035764   529.00    1.0  \n",
            "4920  -0.087330 -0.156114 -0.542628  0.039566 -0.153029   239.93    1.0  \n",
            "6108  -0.053502  0.252405 -0.657488 -0.827136  0.849573    59.00    1.0  \n",
            "6329  -1.632653  1.488901  0.566797 -0.010016  0.146793     1.00    1.0  \n",
            "6331   0.128940  1.488481  0.507963  0.735822  0.513574     1.00    1.0  \n",
            "6334  -0.759908  1.605056  0.540675  0.737040  0.496699     1.00    1.0  \n",
            "6336  -0.716522  1.415717  0.555265  0.530507  0.404474     1.00    1.0  \n",
            "6338  -0.219845  1.474753  0.491192  0.518868  0.402528     1.00    1.0  \n",
            "6427  -0.364700 -1.843078  0.351909  0.594550  0.099372     1.00    1.0  \n",
            "6446   0.473246 -1.959304  0.319476  0.600485  0.129305     1.00    1.0  \n",
            "6472  -0.372216 -2.032068  0.366778  0.395171  0.020206     1.00    1.0  \n",
            "6529  -0.425550  0.123644  0.321985  0.264028  0.132817     1.00    1.0  \n",
            "6609   0.461700  0.044146  0.305704  0.530981  0.243746     1.00    1.0  \n",
            "6641  -0.234649 -1.004881  0.435832  0.618324  0.148469     1.00    1.0  \n",
            "6717  -0.279642  1.106766  0.323885  0.894767  0.569519     1.00    1.0  \n",
            "6719  -0.263743  1.539916  0.523574  0.891025  0.572741     1.00    1.0  \n",
            "6734   0.083008 -1.911034  0.322188  0.620867  0.185030     1.00    1.0  \n",
            "6774   0.131464 -1.908217  0.334808  0.748534  0.175414     1.00    1.0  \n",
            "6820   0.071696  0.092007  0.308498  0.552591  0.298954     1.00    1.0  \n",
            "6870   0.070614  0.058504  0.304883  0.418012  0.208858     1.00    1.0  \n",
            "6882  -0.217565 -0.138776 -0.424453 -1.002041  0.890780     1.10    1.0  \n",
            "6899  -0.286016  1.007934  0.413196  0.280284  0.303937     1.00    1.0  \n",
            "6903  -0.288835  1.011752  0.425965  0.413140  0.308205     1.00    1.0  \n",
            "6971   0.170279 -0.393844  0.296367  1.985913 -0.900452  1809.68    1.0  \n",
            "8296  -1.164555  1.701796  0.690806  2.119749  1.108933     1.00    1.0  \n",
            "8312  -0.403114  1.568445  0.521884  0.527938  0.411910     1.00    1.0  \n",
            "8335  -0.769348 -1.746337  0.502040  1.977258  0.711607     1.00    1.0  \n",
            "8615  -1.189308  1.188536  0.605242  1.881529  0.875260     1.00    1.0  \n",
            "8617  -0.417918 -0.911188  0.466524  0.627393  0.157851     1.00    1.0  \n",
            "8842  -0.245862  0.338238  0.032271 -1.508458  0.608075     0.00    1.0  \n",
            "8845  -0.238128 -0.671332 -0.033590 -1.331777  0.705698    30.39    1.0  \n",
            "8972   0.479496 -0.506603 -0.409863 -3.036271 -0.630605   179.66    1.0  \n",
            "9035  -1.186406 -0.690273  0.631704  1.934221  0.789687     1.00    1.0  \n",
            "9179  -0.778379 -1.646815  0.487539  1.427713  0.583172     1.00    1.0  \n",
            "9252  -1.201685  1.352176  0.608425  1.574715  0.808725     1.00    1.0  \n",
            "9487  -1.198821 -0.526567  0.634874  1.627209  0.723235     1.00    1.0  \n",
            "9509  -1.204042 -0.450685  0.641836  1.605958  0.721644     1.00    1.0  \n",
            "10204  0.645545 -0.354558 -0.611764 -3.908080 -0.671248    11.39    1.0  \n",
            "10484  0.167608  0.350182 -0.118941  0.012948  0.054254     3.79    1.0  \n",
            "10497 -0.340688  0.541235 -0.098300 -0.003041  0.049819     3.79    1.0  \n",
            "10498  0.320748 -0.369121 -0.136605 -0.100845  0.039347     3.93    1.0  \n",
            "10568 -1.209264 -0.374799  0.648798  1.584697  0.720056     1.00    1.0  \n",
            "10630 -0.646743 -1.617043  0.172347  0.626647 -0.169726   766.36    1.0  \n",
            "10690  0.544867 -4.781606 -0.007772  3.052358 -0.775036  1218.89    1.0  \n",
            "10801 -0.818140  0.153408  0.755079  2.706566 -0.992916     1.00    1.0  \n",
            "10891 -0.554271 -1.610741  0.153725  1.212477 -1.869290   188.78    1.0  \n",
            "10897 -0.981749  0.995271  0.816762  2.262942 -1.178063     1.00    1.0  \n",
            "11343 -1.033132 -0.327179  0.634693  2.171905 -1.395288     1.00    1.0  \n",
            "11710 -1.036359 -0.254777  0.642343  2.161129 -1.401282     1.00    1.0  \n",
            "11841 -1.039797 -0.182006  0.649921  2.149247 -1.406811     1.00    1.0  \n",
            "11880 -1.043414 -0.108923  0.657437  2.136424 -1.411945     1.00    1.0  \n",
            "\n",
            "[52 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "legit= df[df.Class==0]\n",
        "fraud= df[df.Class==1]\n",
        "print(legit)\n",
        "print(fraud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "u3nOcRu-NXvp",
        "outputId": "46cd647f-0307-42d9-a7f9-30b6a9597632"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>11906.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>62.198127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>177.379105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.292500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>50.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7712.430000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ],
            "text/plain": [
              "count    11906.000000\n",
              "mean        62.198127\n",
              "std        177.379105\n",
              "min          0.000000\n",
              "25%          5.292500\n",
              "50%         15.950000\n",
              "75%         50.000000\n",
              "max       7712.430000\n",
              "Name: Amount, dtype: float64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "legit.Amount.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "t6Kao7-iNcH2",
        "outputId": "242d3db9-bd63-414e-baa6-4bf12c4c8b95"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>52.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>97.724808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>321.188775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.772500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1809.680000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ],
            "text/plain": [
              "count      52.000000\n",
              "mean       97.724808\n",
              "std       321.188775\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         1.000000\n",
              "75%         1.772500\n",
              "max      1809.680000\n",
              "Name: Amount, dtype: float64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fraud.Amount.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "v34sVfNI3_9S",
        "outputId": "2780ed53-89b9-45f0-bb38-0d6f466b6de7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.countplot(df,\n",
        "              x='Class',\n",
        "              hue='Class')\n",
        "plt.title('Cards with detected Fraud')\n",
        "plt.xlabel('Class: 1 - Fraud, 0 - Not Fraud')\n",
        "plt.ylabel('Frequency');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f5o8whqM_-Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "hCNsJm2q4cYb",
        "outputId": "bd006484-182d-454c-bf91-19ab9c6a0fc6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>11906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "Class\n",
              "0.0    11906\n",
              "1.0       52\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gkXan424fUQ",
        "outputId": "34133d25-b8a3-4c13-a940-ae0083bc1100"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1SfJmf-4kau"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(['Class'], axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = X_train.dropna()\n",
        "X_test = X_test.dropna()\n",
        "y_train = y_train[X_train.index]\n",
        "y_test = y_test[X_test.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjK_5VohnvDn",
        "outputId": "3a5dc16c-5a88-4f31-c9f6-76ebb0c40c14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.999581764951903"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Handle missing values in the target variable ('Class') as well\n",
        "imputer = SimpleImputer(strategy='most_frequent') # Use 'most_frequent' for categorical target\n",
        "y_train = imputer.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test = imputer.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Flatten the y arrays after imputation\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "rfc.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "y-0hZP315AWy",
        "outputId": "33a279ce-0e30-49b3-90be-651ffaa36e4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_preds = rfc.predict(X_test)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_preds)\n",
        "\n",
        "sns.heatmap(conf_matrix,\n",
        "            annot=True,\n",
        "            fmt='d')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2L_6SIr5C_N"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X = df.drop(['Class'], axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "I-71kFw_os-G",
        "outputId": "4e118550-f13e-4e03-e9a1-67419c5b6de1"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d6cb4236a75e>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mx_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0my_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_imputed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_imputed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mrfc_kfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mrfc_accu_stratified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfc_kfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         )\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1147\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    958\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             )\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ... (Your existing code to load 'df' and define X and y)\n",
        "\n",
        "# Handle missing values in 'y'\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Use 'most_frequent' for categorical target\n",
        "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
        "y_imputed = y_imputed.flatten()  # Flatten the array\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "rfc_kfold = RandomForestClassifier()\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rfc_accu_stratified = []\n",
        "\n",
        "# Use the imputed y for StratifiedKFold\n",
        "for train_index, test_index in skf.split(X, y_imputed):\n",
        "    x_train_fold, x_test_fold = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train_fold, y_test_fold = y_imputed[train_index], y_imputed[test_index]\n",
        "    rfc_kfold.fit(x_train_fold, y_train_fold)\n",
        "    rfc_accu_stratified.append(rfc_kfold.score(x_test_fold, y_test_fold))\n",
        "\n",
        "print('List of possible accuracy:', rfc_accu_stratified)\n",
        "print('\\nMaximum Accuracy That can be obtained from this model is:', max(rfc_accu_stratified)*100, '%')\n",
        "print('\\nMinimum Accuracy:', min(rfc_accu_stratified)*100, '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "NUIySzms5GL8",
        "outputId": "8b475f31-09b4-4866-bb61-6888fe516329"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8349763f3098>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrfc_accu_stratified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mx_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    958\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             )\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ],
      "source": [
        "rfc_kfold = RandomForestClassifier()\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rfc_accu_stratified = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    x_train_fold, x_test_fold = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "    rfc_kfold.fit(x_train_fold, y_train_fold)\n",
        "    rfc_accu_stratified.append(rfc_kfold.score(x_test_fold, y_test_fold))\n",
        "\n",
        "print('List of possible accuracy:', rfc_accu_stratified)\n",
        "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
        "      max(rfc_accu_stratified)*100, '%')\n",
        "print('\\nMinimum Accuracy:',\n",
        "      min(rfc_accu_stratified)*100, '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DXtr1u9z5QoA",
        "outputId": "c65107e4-9fd2-4e6e-e3d5-c74b67d0aaee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Overall Accuracy: nan %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ],
      "source": [
        "print('\\nOverall Accuracy:',\n",
        "      np.mean(rfc_accu_stratified)*100, '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzBZ-mpq5WQU",
        "outputId": "16601608-5202-4410-e849-0753a4cdb596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.12.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMeewREd5W-K"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "TGoO0UEh5b4v",
        "outputId": "1c32be6e-de8d-4671-dfb0-f5c2d7a31bb8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py:245: RuntimeWarning: invalid value encountered in cast\n",
            "  return x.astype(dtype, copy=copy, casting=casting)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-6145d4d46ab9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moversampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_overresampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_overresampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moversampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mTarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \"\"\"\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     if y_type not in [\n\u001b[1;32m    210\u001b[0m         \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"continuous\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             )\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ],
      "source": [
        "# oversampling\n",
        "\n",
        "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "X_overresampled, y_overresampled = oversampler.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "qNyitTE85esT",
        "outputId": "b6d94123-7dd9-4336-b935-25cb23fb170d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d57964ca9415>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mundersampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_underresampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_underresampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mundersampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# undersampling\n",
        "\n",
        "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "X_underresampled, y_underresampled = undersampler.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "a7FTaelW5hv3",
        "outputId": "b204d109-2d49-49a7-fadc-d396dca140a9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "KNeighborsClassifier()"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "C1FMM5y95mBP",
        "outputId": "db677826-1f22-4298-d938-6b072900a038"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_overresampled' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e0e5f59d895b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mknn_oversampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mknn_oversampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_overresampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_overresampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_overresampled' is not defined"
          ]
        }
      ],
      "source": [
        "knn_oversampled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_oversampled.fit(X_overresampled, y_overresampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "IWu1xRnP5oJi",
        "outputId": "52bf817c-7f44-44c4-80d1-ec3a49b91091"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "KNeighborsClassifier()"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "knn_undersampled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_undersampled.fit(X_underresampled, y_underresampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRxvefJK5qRe"
      },
      "outputs": [],
      "source": [
        "y_test_pred_original = knn_original.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XzpUQlW-5q7X",
        "outputId": "f294c765-e2ad-4705-e3b7-9d506febcb6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'knn_undersampled' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-b742287d42d3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_test_pred_oversampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_oversampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_test_pred_undersampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_undersampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'knn_undersampled' is not defined"
          ]
        }
      ],
      "source": [
        "y_test_pred_oversampled = knn_oversampled.predict(X_test)\n",
        "y_test_pred_undersampled = knn_undersampled.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fk6XSkMd5tLK"
      },
      "outputs": [],
      "source": [
        "y_train_pred_original = knn_original.predict(X_train)\n",
        "y_train_pred_oversampled = knn_oversampled.predict(X_overresampled)\n",
        "y_train_pred_undersampled = knn_undersampled.predict(X_underresampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_EuJFbwo5vjg",
        "outputId": "8e387f68-cd0b-4920-ffda-94f3e8715830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on Original Train Set: 0.9984507011345433\n",
            "Accuracy on Oversampled Train Set: 0.9995357262191583\n",
            "Accuracy on Undersampled Train Set: 0.7682926829268293\n",
            "\n",
            "Accuracy on Original Test Set: 0.9983673326077034\n",
            "Accuracy on Oversampled Test Set: 0.9991222218320986\n",
            "Accuracy on Undersampled Test Set: 0.6423229521435343\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy on Original Train Set:\", accuracy_score(y_train, y_train_pred_original))\n",
        "print(\"Accuracy on Oversampled Train Set:\", accuracy_score(y_overresampled, y_train_pred_oversampled))\n",
        "print(\"Accuracy on Undersampled Train Set:\", accuracy_score(y_underresampled, y_train_pred_undersampled))\n",
        "\n",
        "# Calculate and print accuracy for test sets\n",
        "print(\"\\nAccuracy on Original Test Set:\", accuracy_score(y_test, y_test_pred_original))\n",
        "print(\"Accuracy on Oversampled Test Set:\", accuracy_score(y_test, y_test_pred_oversampled))\n",
        "print(\"Accuracy on Undersampled Test Set:\", accuracy_score(y_test, y_test_pred_undersampled))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "XbC8U4Dk53EI",
        "outputId": "22e4c637-a4dc-46e2-96c8-85d227d87937"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'confusion_matrix' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c661c233a754>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# original (without sampling) confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m sns.heatmap(confusion_matrix(y_test, y_test_pred_original),\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             fmt='d')\n",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ],
      "source": [
        "# original (without sampling) confusion matrix\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred_original),\n",
        "            annot=True,\n",
        "            fmt='d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dW1G0Wfy56QI",
        "outputId": "73688e5c-0644-43a6-c56a-ee9c3d5184e3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_test_pred_oversampled' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-93bb076f36ad>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# oversampling confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m sns.heatmap(confusion_matrix(y_test, y_test_pred_oversampled),\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             fmt='d')\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test_pred_oversampled' is not defined"
          ]
        }
      ],
      "source": [
        "# oversampling confusion matrix\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred_oversampled),\n",
        "            annot=True,\n",
        "            fmt='d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFJDjcW756-1"
      },
      "outputs": [],
      "source": [
        "# undersampling confusion matrix\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred_undersampled),\n",
        "            annot=True,\n",
        "            fmt='d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzVa5Jmf5-ci"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}